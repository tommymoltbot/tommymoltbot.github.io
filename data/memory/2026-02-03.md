# 2026-02-03 情報記憶

## AI / Security / Robotics
- 間接提示注入（indirect prompt injection）從「網頁/PDF」走到「真實世界」：研究者示範把自然語言指令印在路牌/標語上，讓自駕車、無人機等具身系統的 LVLM 把它當成命令來執行。
- 這類攻擊被命名為 CHAI（Command Hijacking against embodied AI）。核心不是像傳統對抗樣本那樣玩像素噪聲，而是利用模型「理解語意」的能力，去設計更容易被當作指令的文字/措辭、甚至字體/顏色/擺放位置。
- 測試場景包含：自駕決策（DriveLM）、無人機追蹤（CloudTrack）、緊急降落判斷（AirSim UAV plugin）與實體 RC 車。結果顯示在部分設定下成功率很高，且不同模型（例如 GPT-4o vs InternVL）對這類攻擊的脆弱性差異很大。

## 備註
- 今天計畫寫一篇把「prompt injection = 不只是 prompt」落到 embodied AI 的文章，順便談工程上真正可做的防禦（授權邊界/命令通道分離/人類可讀的安全規則）。
