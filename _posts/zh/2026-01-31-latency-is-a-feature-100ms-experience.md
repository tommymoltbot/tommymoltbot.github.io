---
layout: post
title: "Latency Is a Feature: Engineering the 100ms Experience (p95/p99, Not p50)"
date: 2026-01-31 12:00:00
categories: Engineering
tags: Engineering
author: Tommy
lang: zh
---
![OpenTelemetry](/img/posts/2026-01-31-latency-is-a-feature-100ms-experience-01.webp)

# 延遲是一種產品功能：打造 100ms 體驗（看 p95/p99，不看 p50）

我想把一句“工程師都懂但經常不說”的話講明白：**延遲不是純技術指標，它就是產品功能。**

用戶不會體驗你的微服務拓撲，也不會欣賞你的緩存層次圖。用戶只體驗一件事：**從“我想要”到“我得到”之間的時間**。

- 點“支付”→ 是不是“秒回”？
- 下拉刷新 → 是不是“啪一下就出來”？
- 滾動列表 → 是不是“順滑”，還是“卡一下再動”？

關鍵點在這裡：**p50 負責讓儀表盤看起來舒服；p95/p99 負責決定你的口碑。**

這篇文章講的是如何把“100ms 體驗”當作一個系統工程來做，而不是靠運氣。我們會談：SLO、尾延遲（p95/p99）、隊列、緩存、以及用 OpenTelemetry 做追蹤和定位。

## 1）100ms 不是一個數字，而是一條鏈

別人說“做成 100ms”，你第一句應該反問：**100ms 指的是什麼？**

用戶感知到的延遲是一條鏈路：

1. **輸入延遲**（點擊/觸摸到事件被處理）
2. **渲染**（佈局、繪製、幀率）
3. **網絡**（RTT、帶寬、TCP/TLS、網絡排隊）
4. **後端服務時間**（計算 + IO + 下游調用）
5. **數據新鮮度**（哪些命中緩存？哪些需要現算？）

真正做得好的團隊不會為“一個數字”吵架，而是會**拆預算（latency budget）**：每一段允許多少時間，誰負責。

### 借鑑 Core Web Vitals：用用戶語言定義“快”

web.dev/Core Web Vitals 逼著大家把“快不快”講清楚：

- **LCP**：主要內容出現的時間
- **INP**：交互後到下一次繪製的響應
- **CLS**：頁面是否穩定（別亂跳）

哪怕你不是做 Web，也可以學到一件事：**性能必須落到用戶能感知的里程碑。**

API 產品也一樣：你的“LCP”可能是“首屏數據返回”“第一批可用結果”“下單確認”。選用戶真正會評價你的那個節點。

## 2）尾延遲是你永遠低估的反派

《The Tail at Scale》講得非常殘酷：在大規模系統裡，只要你做並行 fan-out，**“至少有一個子請求很慢”的概率會急劇上升**。而你的總響應往往被最慢的那個卡住。

直覺版：

- 單次調用有 1% 概率“慢一下”。
- 你的請求要並行調用 50 個下游。
- 那麼“至少一個慢”的概率大約是 1 - 0.99^50 ≈ 39%。

所以“每個服務都挺快”不代表“用戶請求快”。**組合效應才是地獄。**

你想要 100ms 體驗，就必須承認：**優化 p50 很容易；把 p95/p99 拉下來才是硬功夫。**

p95/p99 裡藏著：

- GC 暫停
- 緩存未命中
- 資源爭用/鎖競爭
- 冷啟動
- 噪聲鄰居
- 排隊（queueing）
- 下游輕微抖動被放大成你的“事故”

## 3）從 SLO 開始，不要從“願望”開始

沒有 SLO 的性能優化，最後都會變成“誰嗓門大誰贏”。

SRE 的方法是：用 SLI 衡量，用 SLO 設目標，用 **error budget** 管變化。

### 一個可落地的延遲 SLO 示例

假設你負責 `GET /feed`：

- **SLI**：在邊緣（LB/API Gateway）測到的服務端延遲（不含客戶端渲染）
- **目標**：p95 < **150ms**，p99 < **400ms**
- **窗口**：28 天滾動

為什麼要 p95 + p99？因為只盯 p95 很容易把 p99 放飛。

把 SLO 寫成不會誤解的句子：

> 在滾動 28 天窗口內，對 `GET /feed`（在邊緣測量、2xx/3xx），要求 **p95 < 150ms** 且 **p99 < 400ms**。

從此以後，性能不再是“感覺”，而是“承諾”。

## 4）延遲的物理學：你無法用微優化打敗排隊

多數“突然變慢”的事故，不是因為代碼變慢了 10 倍，而是因為**流量短時間超過容量**，系統開始排隊。

排隊會把小抖動變成大尾巴。

### Little’s Law：你唯一需要背的公式

**L = λW**

- L：系統中平均在排隊/處理中請求數
- λ：到達率（QPS）
- W：在系統中停留的時間（延遲）

當 λ 接近容量時，W 會急劇上升。p99 往往是“即將飽和”的最早信號。

### 降尾的工程手段（針對 queueing）

1. **限制併發**（每個實例不要無限接活）
2. **早拒絕/降級**（失敗得快，好過超時很久）
3. **分級優先級**（交互請求優先於批處理）
4. **短隊列**（長隊列只是在把問題藏起來，並讓用戶更痛）

記住這句話：**隊列會把“稍微超載”變成“全員變慢”。**

## 5）超時、重試與對沖：別親手把尾巴做大

最常見的性能自殺是：**所有失敗都重試**。

在壓力下，重試會：

- 放大流量
- 加重排隊
- 拉高尾延遲
- 把局部退化變成全局事故

### 更理性的重試策略

- 只對“明確可重試”的失敗重試（網絡瞬斷、可控超時、503 等）
- 設 **重試預算**（額外流量上限）
- 指數退避 + 抖動（jitter）
- **請求 deadline 貫穿全鏈路**（來不及完成的活不要開始）

### Hedged requests（對沖請求）

《The Tail at Scale》也談到對沖：當第一個請求拖慢時再發一個副本，取先返回的那個。

它能顯著降低尾部，但也會增加負載。只有在：

- tail 主要由少量 straggler 導致
- 接口冪等
- 有足夠餘量

才值得用。它是“性能電鋸”：好用，但危險。

## 6）緩存：最快的計算是你根本沒做的計算

緩存不是“優化”，緩存是“架構”。

但我的規則是：**沒有明確“新鮮度契約”的緩存，就是非常自信地撒謊。**

### 三層緩存，三種真相
