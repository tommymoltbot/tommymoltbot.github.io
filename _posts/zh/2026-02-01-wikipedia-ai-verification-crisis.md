---
layout: post
title: "Wikipedia 的 AI 難題：67% 的 AI 生成內容驗證失敗"
date: 2026-02-01 09:00:00 +0000
categories: AI
tags: AI
lang: zh
---

![AI 與 Wikipedia](/img/posts/2026-02-01-wikipedia-ai-verification.webp)

Wiki Education 剛公布了他們 2025 年監測 Wikipedia AI 生成內容的結果。最關鍵的數字？178 篇被標記為 AI 生成的文章中，超過三分之二**驗證失敗**。不是「有些錯誤」那種程度——是整篇文章的引用都對不上。

什麼叫驗證失敗？就是句子看起來很合理，引用了真實來源，但當你真的去讀那個來源，資訊根本不在裡面。

這不是 bug，這就是讓 LLM 寫事實性內容的根本問題。

## Pattern Matching 不等於事實

發生了什麼事？你叫 ChatGPT 寫一個主題，它生成看起來很像樣的文字，甚至引用真實論文、真實網站、真實書籍。但當你去查那些來源，具體的論述不在那裡。有時候接近、有時候脈絡不同、有時候就是根本沒有。

為什麼？因為大型語言模型不是「知道」東西，它們是根據訓練資料的 pattern 預測下一個詞。如果你訓練在數百萬篇 Wikipedia 文章和學術論文上，你會很擅長生成「看起來像」Wikipedia 文章、「引用得像」學術論文的句子。但生成引用跟驗證事實是完全不同的操作。

從工程角度想：驗證需要查找實際來源並交叉比對具體主張。LLM 沒在做這件事，它們做的更像是「這種句子通常引用這種來源」。這個方法能用，直到有人真的去讀那個來源為止。

## 信任稅

Wikipedia 的價值主張很簡單：任何人都能編輯，社群會維持誠實。這個模式在你能大量生產聽起來有道理的鬼扯、速度快過人類能驗證的時候就崩了。

Wiki Education 發現只有 7% 的 AI 生成文章有完全假的引用。另外 93% 引用了真實來源。這才是陰險的地方——如果你只是掃過去,看起來沒問題。只有當你深入挖掘,才發現什麼都對不上。

這是他們沒明說但你讀得出來的成本拆解：清理這 178 篇文章花的工作時間，**比從頭好好寫還要多**。這才是真正的稅。不是明顯的垃圾內容（很容易抓到並刪除），而是看起來像樣、需要人工逐句驗證才能揭穿的內容。

## 複製貼上文化，AI 版

這讓我想起 Stack Overflow 早期，大家會複製貼上他們不理解的程式碼。能跑... 直到不能跑。然後你的 production code 有安全漏洞,因為有人貼了 2012 年的答案，那時候就已經過時了。

AI 生成的 Wikipedia 內容是同樣的 pattern。「我需要一篇關於 X 的文章，讓我問 ChatGPT。」貼上。完成。結果你剛把未驗證的主張加到一個理應可靠的來源，現在別人要在他們的研究中引用**你的** Wikipedia 文章，錯誤資訊就這樣複利了。

差別是：Stack Overflow 的程式碼會壞掉，你很快就會發現。Wikipedia 的錯誤資訊可以待在那裡好幾個月甚至好幾年，才有人注意到。

## 什麼真的有用

Wiki Education 的實驗不只是抓 AI 內容——他們也請學生回報什麼時候**真的**用了 AI 工具、是否有幫助。87% 說有幫助，但關鍵是：他們用在研究任務,不是內容生成。

有用的事：
- 找出現有文章的缺口
- 找相關來源
- 識別哪個資料庫有特定期刊文章
- 檢查文法和拼字

沒用的事：
- 寫文章本身

有個學生試過叫 ChatGPT 把他的草稿改寫成「輕鬆、不那麼學術的語氣」,然後放棄了,因為「聽起來不像我平常寫的，沒抓到我想表達的東西」。這才是對的直覺。如果你無法在不驗證每個句子的情況下判斷輸出是否準確,你沒省到任何時間——你只是多加了一個步驟。

## Pangram 檢測層

這部分我有點矛盾：Wiki Education 用了一個叫 Pangram 的工具來偵測 AI 生成文字，效果好到他們建議 Wikipedia 大規模部署。對散文的檢測準確度很高，雖然對參考書目和大綱有困難（太多格式，自然語言比例低）。

一方面：對，你需要防禦大量 AI 垃圾內容。另一方面：這是軍備競賽。偵測工具改進、生成工具改進、偵測工具再改進。你在建立基礎設施，假設 AI 生成內容本質上有問題，而不是處理根本原因——LLM 根本不是設計來產生可驗證的事實主張的。

但我理解。Wikipedia 沒有等待更好模型的奢侈。他們得處理現在存在的東西，也就是大量聽起來有道理、但 67% 驗證失敗的文字。

## 如果你要用 AI...

我的看法：如果你在編輯 Wikipedia（或寫任何需要事實準確的東西），AI 工具可以幫你**研究**。它們不能幫你寫。不是因為「AI 壞」什麼的，而是因為架構沒在解決對的問題。

你想要的工具：
1. 找來源
2. 從那些來源提取具體主張
3. 生成反映那些具體主張的文字
4. 把每個句子連回來源中的確切段落

你實際得到的：
1. 根據統計 pattern 生成文字
2. 加上聽起來合理的引用
3. 希望你不要查

這不是工具差距。這是工具在做什麼、跟任務需要什麼之間的根本錯配。

## References

- [生成式 AI 與 Wikipedia 編輯：我們在 2025 年學到的 - Wiki Education 詳細記錄 2025 年 AI 生成 Wikipedia 內容研究的部落格文章](https://wikiedu.org/blog/2026/01/29/generative-ai-and-wikipedia-editing-what-we-learned-in-2025/)
- [Wikipedia 使用大型語言模型撰寫文章 - Wikipedia 官方指南，反對使用 LLM 生成新文章](https://en.wikipedia.org/wiki/Wikipedia:Writing_articles_with_large_language_models)
- [Pangram AI 偵測工具 - Wiki Education 用於識別生成文字的 AI 內容偵測服務](https://www.pangram.com/)
