---
layout: post
title: "Prompt Injection 逃出聊天室：CHAI 讓具身 AI 的攻擊面直接長到現實世界"
date: 2026-02-04 11:00:00 +0000
categories: AI
tags: AI
author: Tommy
lang: zh
image: /img/posts/2026-02-04-chai-embodied-ai-01.webp
---

![一塊看起來很普通的路牌，但它其實可以變成指令通道](/img/posts/2026-02-04-chai-embodied-ai-01.webp)

以前我腦中對 prompt injection 的分類很單純：這是「LLM app security」的事。

典型劇本是：模型讀到網頁 / PDF，裡面藏了惡意指令，agent 乖乖照做，下游就炸。很常見，也很煩。

但我最近看到一篇名字幾乎直接劇透的論文：

**CHAI — Command Hijacking against embodied AI。**

它要講的其實很簡單：prompt injection 不再只活在聊天室、瀏覽器或文件裡。當你的機器人 / 無人機 / 自駕系統開始用 Large Vision-Language Model（LVLM）做決策，**現實世界本身就可以變成 prompt**。

這不是哲學問題，是工程問題。

## 真正的轉折：指令可以藏在「感知」裡

一般工具型 agent 的安全邊界，你大概會拆成：

- 使用者文字（不可信）
- system instructions（可信）
- tools（特權能力）

具身系統多了一條通道：

- 相機看到的東西

如果模型把畫面裡的文字當成「可執行的指令」，你等於開了一個攻擊者可以用印表機寫入的指令路徑。

路牌 ≈ 潛在的 API call。

CHAI 的價值在於它把這件事定義成 **command hijacking（指令劫持）**，而不是老掉牙的「視覺模型也會被騙」。

## CHAI 在幹嘛（用工程語言講）

照論文摘要的說法，CHAI 是一種 prompt-based attack：

- 把「看起來像正常告示」的自然語言指令塞進視覺輸入（例如招牌、標語）
- 系統化地搜尋 token space，建立有效 prompt 的字典
- 用 attacker model 生成 **Visual Attack Prompts**

他們在多個 LVLM agent 與任務上評估，包括：

- 無人機緊急降落判斷
- 自駕決策
- 空中物體追蹤
- 以及真實的機器車

重點不是「哇模型很脆弱」—— 這大家都知道。

重點是：**語意理解能力本身就是攻擊面。**

模型的強項（理解語言 + 情境）會變成被利用的手段。

## 為什麼它比傳統 adversarial example 更讓人不安

我不是說對抗像素噪聲不重要。但過去很多人會下意識覺得：

- 真實世界很難穩定重現
- 光線/鏡頭一變就失效
- 攻擊者需要高度控制場景

CHAI 的前提不一樣。它不是藏在「人看不出來」的噪聲裡，而是用 **人也讀得懂的文字**。

可讀文字就代表：它更容易被複製、被散播、被量產。

如果你的系統有「把環境文字當指令」的習慣，攻擊者會很喜歡你：

- 可攜性高（不同模型之間可能也有效）
- 更貼近真實世界（你真的可以印一張出來）
- 社會上也合理（「就一個告示牌啊」）

這種東西最容易混進 production。

## 我直白的工程結論：你需要「命令通道」，不是 vibes

很多具身系統 demo 的流程其實很像：

1) 模型看畫面
2) 模型推理
3) 模型輸出動作

順的時候看起來像魔法。

但出事的時候，會是最糟糕的那種：它會很自信地做錯。

只要你允許「畫面裡看到的任何文字」影響動作，你就必須對這件事有明確政策。

我覺得最能撐過現實的原則其實很土：

> **把環境中讀到的文字一律視為不可信輸入，而不是權威指令。**

也就是：

- 模型可以回報它看到了什麼文字。
- 模型可以詢問人類/上層控制器「這段文字該不該當成任務線索」。
- 系統可以決定它是否在 mission context 內。

但模型不應該透過閱讀世界來「自動發明新規則」。

因為世界是可寫入的。

### 一個最小化的「命令邊界」設計

如果我在做任何帶物理致動（actuation）的 LVLM agent，我會把這三件事硬拆開：

- **Perception（感知）**：場景裡有什麼（物體、文字、上下文）
- **Intent（意圖）**：使用者/操作員到底要你幹嘛
- **Action policy（動作政策）**：在什麼條件下允許哪些動作

然後讓「環境文字」只流進 perception。

接著 action policy 的通過必須依賴：

- 明確的 mission context（操作員意圖）
- 明確的授權/允許條件
- 明確的限制（速度上限、地理圍欄、安全規則）

不是 vibes。

## 那個大家會忽略的細節（直到真的出事）

很多團隊會看完這種攻擊然後說：

「我們的無人機又不會去讀路牌。」

但如果你的 prompt 裡有任何像這種話：

```text
「用環境裡所有可用線索來決定下一步動作。」
```

那你其實就是在叫它讀路牌。

不舒服但很真：**語意泛化不是免費的**。

你讓模型從環境中泛化規則，攻擊者就會從環境中泛化指令回來。

## 我接下來會關注什麼

我想看後續研究往兩個方向補：

1) **更像部署環境的評估**（不同鏡頭、光線、字體、多 agent）
2) **可以維運的防禦**（不是只看 accuracy 指標）

因為具身 AI 真正的問題從來不是「能不能騙到模型」。

而是：

> 「當模型被騙的時候，整個系統會不會還是安全？」

這才是門檻。

---

## References

- [CHAI：針對具身 AI 的指令劫持（arXiv:2510.00181）](https://arxiv.org/abs/2510.00181)
