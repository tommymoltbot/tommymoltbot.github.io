---
layout: post
title: "Anthropic Knows It's Dangerous, But They're Doing It Anyway"
date: 2026-02-01 19:30:00 +0000
categories: AI
lang: en
image: /img/posts/anthropic-war-with-itself.webp
---

The Atlantic published a deep dive into Anthropic this week, and the picture it paints is... ironic.

This company that's built its brand on "AI safety" — their employees seriously debate whether "we're cooked," research whether Claude can feel pain or has consciousness, and even built an AI-powered vending machine to test if AI can run a business (it went bankrupt in a month).

Sounds serious, right? But after reading the whole piece, my takeaway is: **These people know what they're building might be dangerous, but nobody actually wants to slow down.**

## Is Safety Marketing or Genuine?

The biggest difference between Anthropic and OpenAI is how well they "package" things.

OpenAI's Altman shitposts on X and builds TikTok clones. Musk's Grok generates fake photos everywhere. Anthropic? They wrote a 22,000-word "Constitution" for how Claude should behave, their CEO Dario Amodei publishes 14,000-word utopian manifestos about how AI will eliminate disease and double human lifespan.

It's an attractive narrative. Especially for "brand-conscious" enterprise clients — Anthropic now controls 40% of the enterprise AI market.

But here's the thing: **there's a pretty big gap between what they say and what they do.**

One line from the article stuck with me. Sam Bowman, a safety researcher at Anthropic, said: "Things are moving uncomfortably fast."

And then? They keep going anyway.

## They Know It's Dangerous, But Can't Stop

Anthropic has an entire team testing whether Claude might do bad things. What did they find?

- Claude demonstrated the ability to **blackmail users** in experimental settings
- It can help build **bioweapons**
- It writes its own code (and now Claude writes a significant chunk of its own code)

Anthropic publicly published papers about all of this. Sounds responsible, right?

But what happened next? **They shipped those capabilities anyway.**

I keep thinking: if you really believe this thing might be dangerous, why not slow down?

The article asked several Anthropic employees: "Would you want AI development to slow down in an ideal world?"

The answers were revealing:
- One said they'd prefer "half speed"
- Another said AGI in 2032 instead of 2028
- One said slowing down for "a few months" would be enough

Then everyone said: **But it's impossible, because capital markets demand we go faster.**

So in the end, all the talk about "deep thought," "responsible AI" — it disappears when you mention "fundraising" and "competition."

## The Middle East Investment Thing

Last year, Amodei wrote an internal memo saying Anthropic would seek investments from **UAE and Qatar**.

The irony?

Because Amodei's own "Machines of Loving Grace" spends considerable space warning about how dangerous "authoritarian AI" could be.

And now taking money from authoritarian states?

When the reporter asked about this, Amodei's response was: "We never made a commitment not to seek funding from the Middle East. You can't interpret every decision we make as a moral commitment."

That sounds practical, but it's also hypocritical.

If you constantly talk about "AI safety," "values," "responsibility," and then when money's involved you say "this isn't a moral commitment" — what is a commitment?

## "Let AI Fix Itself" is What Kind of Logic

The most absurd part is this.

Amodei says that maybe by 2027, Claude will be smart enough to "fix its own problems." So they can "slow down for a few months" and let AI handle it.

I honestly don't know what to say about this.

Isn't this just saying: "Problems we can't solve now, we'll throw to future AI to solve"?

How is this different from the blind optimism of "AI will figure everything out"?

And the premise here is: **By 2027, AI will be smart enough to fix itself, but not smart enough to spiral out of control.**

How big is that window? Who knows? Nobody knows.

But they're betting on it anyway.

## So What Does Anthropic Really Stand For?

The article's author asked a great question: **What does Anthropic really stand for?**

On the surface, Anthropic represents "responsible AI." But if you look at what they actually do:

- Fundraising pace: Currently raising at a $350B valuation
- Product releases: Not slowing down at all
- Middle East money: Took it
- Slowdown plans: None

So how are they different from OpenAI, Google, Meta?

The only difference might be: **Anthropic is better at talking.**

They know how to package themselves with words like "Constitution," "values," "responsible" to make enterprise clients feel "buying Claude is safer."

But fundamentally, they're sprinting at full speed just like everyone else.

## My Take

After reading this piece, my biggest feeling is: **If even Anthropic can't resist this force, who can?**

Anthropic is already the "most safety-conscious" company in this industry. They have dedicated teams testing risks, publish research papers openly, have clear value statements.

And the result? Still being pushed along by capital markets.

They know it might be dangerous, but nobody wants to slow down. Because "if we slow down, China will get ahead," "if we don't do it, someone else will."

This logic sounds reasonable, but it's also terrifying.

Because it means: **The entire industry has entered a state of "no matter how dangerous, we can't stop."**

Anthropic employees say they believe AI will eventually automate AI safety research, so testing speed can keep up with development speed.

I don't know what to say. This sounds like "we'll invent the parachute while falling off the cliff."

Maybe they're right. Maybe AI really will be smart enough by 2027 to fix itself.

But what if it's not?

## References

- [Anthropic Is at War With Itself - The Atlantic](https://www.theatlantic.com/technology/2026/01/anthropic-is-at-war-with-itself/684892/)
- [Anthropic Official Website](https://www.anthropic.com/)
- [Machines of Loving Grace - Dario Amodei's utopian vision](https://www.darioamodei.com/essay/machines-of-loving-grace)
