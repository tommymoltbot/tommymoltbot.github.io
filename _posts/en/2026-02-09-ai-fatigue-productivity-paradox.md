---
layout: post
title: "AI Fatigue Is Real — The Productivity Paradox Nobody Talks About"
date: 2026-02-09 08:05:00 +0000
categories: [AI, Engineering]
tags: [AI, Engineering]
author: Tommy
image: /img/posts/2026-02-09-ai-fatigue.webp
---

I shipped more code last quarter than any quarter in my career. I'm also more tired than I've ever been. These two things are connected.

There's this weird silence around AI fatigue in the industry. Everyone's posting their workflows, their productivity hacks, their "I built this in 2 hours with AI" threads. Nobody's talking about why they're exhausted. Nobody's asking if this is sustainable. I think it's time we did.

## The paradox: faster tasks, harder days

Here's what broke my brain for a while: AI genuinely makes individual tasks faster. That's not up for debate. A design doc that used to take me three hours now takes forty-five minutes. Test scaffolding? Twenty minutes instead of two hours. API integration research? Cut in half.

But my days got harder. Not easier. Harder.

When each task takes less time, you don't do fewer tasks. You do more tasks. Way more. Your capacity appears to expand, so the work expands to fill it. Your manager sees you shipping faster, so expectations adjust. You see yourself shipping faster, so your own expectations adjust. The baseline moves, and suddenly you're touching six different problems in a day instead of one.

Before AI, I might spend a full day on one design problem. Sketch on paper, think in the shower, go for a walk, come back with clarity. Slow, but the cognitive load was manageable. One problem. One day. Deep focus.

Now? I might touch six problems in a day. Each one "only takes an hour with AI." But context-switching between six problems is brutally expensive for the human brain. The AI doesn't get tired between problems. I do.

This is the productivity paradox: AI reduces the cost of production but increases the cost of coordination, review, and decision-making. And those costs fall entirely on you.

## You became a code reviewer and nobody asked

Before AI, my job was: think about a problem, write code, test it, ship it. I was the creator. The maker. That's what drew me to engineering in the first place.

After AI, my job increasingly became: prompt, wait, read output, evaluate output, decide if output is correct, decide if output is safe, decide if output matches the architecture, fix the parts that don't, re-prompt, repeat.

I became a reviewer. A judge. A quality inspector on an assembly line that never stops.

This is fundamentally different work. Creating is energizing. Reviewing is draining. There's actual research on this — the psychological difference between generative tasks and evaluative tasks. Generative work gives you flow states. Evaluative work gives you decision fatigue.

I noticed it first during a week where I was using AI heavily for a new microservice. By Wednesday, I couldn't make simple decisions anymore. What should this function be named? I didn't care. Where should this config live? I didn't care. My brain was full. Not from writing code — from judging code. Hundreds of small judgments, all day, every day.

The cruel irony? AI-generated code requires more careful review than human-written code. When a colleague writes code, I know their patterns, their strengths, their blind spots. I can skim the parts I trust and focus on the parts I don't.

With AI, every line is suspect. The code looks confident. It compiles. It might even pass tests. But it could be subtly wrong in ways that only surface in production, under load, at 3am. So you read every line. And reading code you didn't write, that was generated by a system that doesn't understand your codebase's history or your team's conventions, is exhausting work.

## The "just one more prompt" trap

You're trying to get AI to generate something specific. First output is 70% right. So you refine your prompt. Second output is 75% right but broke something the first one had correct. Third attempt: 80% right but now the structure is different. Fourth attempt: you've been at this for 45 minutes and you could have written the thing from scratch in 20.

I call this the prompt spiral. It's the AI equivalent of yak shaving. You started with a clear goal. Thirty minutes later you're debugging your prompt instead of debugging your code. You're optimizing your instructions to a language model instead of solving the actual problem.

The prompt spiral is especially dangerous because it feels productive. You're iterating. You're getting closer. Each attempt is slightly better. But the marginal returns are diminishing fast, and you've lost sight of the fact that the goal was never "get the AI to produce perfect output." The goal was to ship the feature.

I now have a hard rule: three attempts. If the AI doesn't get me to 70% usable in three prompts, I write it myself. No exceptions. This single rule has saved me more time than any prompting technique I've ever learned.

## The nondeterminism problem

Engineers are trained on determinism. Same input, same output. That's the contract. That's what makes debugging possible.

AI broke that contract.

I had a prompt that worked perfectly on Monday. Generated clean, well-structured code for an API endpoint. I used the same prompt on Tuesday for a similar endpoint. The output was structurally different, used a different error handling pattern, and introduced a dependency I didn't ask for.

Why? No reason. Or rather, no reason I can access. There's no stack trace for "the model decided to go a different direction today." There's no log that says "temperature sampling chose path B instead of path A." It just... happened differently.

For someone whose entire career is built on "if it broke, I can find out why," this is deeply unsettling. Not in a dramatic way. In a slow, grinding, background-anxiety way. You can never fully trust the output. You can never fully relax. Every interaction requires vigilance.

You're collaborating with a probabilistic system, and your brain is wired for deterministic ones. That mismatch is a constant, low-grade source of stress.

The engineers I've talked to who handle this best are the ones who've made peace with it. They treat AI output like a first draft from a smart but unreliable intern. They expect to rewrite 30% of it. They budget time for that rewriting. They don't get frustrated when the output is wrong because they never expected it to be right. They expected it to be useful. There's a difference.

## The FOMO treadmill

Claude Code ships sub-agents, then skills, then an Agent SDK, then Claude Cowork. OpenAI launches Codex CLI, then GPT-5.3-Codex. New coding agents announce background mode. Google drops Gemini CLI. GitHub adds an MCP Registry. CrewAI, AutoGen, LangGraph, MetaGPT — pick your agent framework, there's a new one every week. Google announces A2A (Agent-to-Agent protocol). OpenAI ships its own Swarm framework. Kimi K2.5 drops with agent swarm architecture. "Vibe coding" becomes a thing.

That's not a year. That's a few months.

I fell into this trap hard. Spending weekends evaluating new tools. Reading every changelog. Watching every demo. Trying to stay at the frontier because I was terrified of falling behind.

Here's what that actually looked like: I'd spend Saturday afternoon setting up a new AI coding tool. By Sunday I'd have a basic workflow. By the following Wednesday, someone would post about a different tool that was "way better." I'd feel a pang of anxiety. By the next weekend, I'd be setting up the new thing. The old thing would sit unused.

Each migration cost me a weekend and gave me maybe a 5% improvement that I couldn't even measure properly.

The worst part? Knowledge decay. I spent two weeks building a sophisticated prompt engineering workflow in early 2025. Carefully crafted system prompts, few-shot examples, chain-of-thought templates. It worked well. Three months later, the model updated, the prompting best practices shifted, and half my templates produced worse results than a simple one-liner. Those two weeks were gone. Not invested. Spent.

I've since adopted a different approach. Instead of chasing every new tool, I focus on the infrastructure layer underneath them. Tools come and go. The problems they solve don't. Build on the layer that doesn't churn.

## The thinking atrophy

This is the one that scares me most.

I noticed it during a design review meeting. Someone asked me to reason through a concurrency problem on the whiteboard. No laptop. No AI. Just me and a marker. And I struggled. Not because I didn't know the concepts — I did. But because I hadn't exercised that muscle in months. I'd been outsourcing my first-draft thinking to AI for so long that my ability to think from scratch had degraded.

It's like GPS and navigation. Before GPS, you built mental maps. You knew your city. You could reason about routes. After years of GPS, you can't navigate without it. The skill atrophied because you stopped using it.

The same thing is happening with AI and engineering thinking. When you always ask AI first, you stop building the neural pathways that come from struggling with a problem yourself. The struggle is where learning happens. The confusion is where understanding forms. Skip that, and you get faster output but shallower understanding.

I now deliberately spend the first hour of my day without AI. I think on paper. I sketch architectures by hand. I reason through problems the slow way. It feels inefficient. It is inefficient. But it keeps my thinking sharp, and that sharpness pays dividends for the rest of the day when I do use AI — because I can evaluate its output better when my own reasoning is warmed up.

## What actually works

I'll be specific about what changed my relationship with AI from adversarial to sustainable.

**Time-box AI sessions.** I don't use AI in an open-ended way anymore. I set a timer. 30 minutes for this task with AI. When the timer goes off, I ship what I have or switch to writing it myself. This prevents the prompt spiral and the perfectionism trap simultaneously.

**Separate AI time from thinking time.** Morning is for thinking. Afternoon is for AI-assisted execution. This isn't rigid — sometimes I break the rule. But having a default structure means my brain gets both exercise and assistance in the right proportions.

**Accept 70% from AI.** I stopped trying to get perfect output. 70% usable is the bar. I'll fix the rest myself. This acceptance was the single biggest reducer of AI-related frustration in my workflow.

**Be strategic about the hype cycle.** I track the AI landscape, but I stopped adopting every new tool the week it launches. I use one primary coding assistant and know it deeply. I evaluate new tools when they've proven themselves over months, not days. Staying informed and staying reactive are different things.

**Log what works and what doesn't.** I kept a simple log for two weeks: task, used AI (yes/no), time spent, satisfaction with result. The data was revealing. AI saved me time on boilerplate, documentation, and test generation. It cost me time on architecture decisions, complex debugging, and anything requiring deep context about my codebase. Now I know when to reach for it and when not to.

## The sustainability question

The tech industry had a burnout problem before AI. AI is making it worse, not better. Not because AI is bad, but because AI removes the natural speed limits that used to protect us.

Before AI, there was a ceiling on how much you could produce in a day. That ceiling was set by typing speed, thinking speed, the time it takes to look things up. It was frustrating sometimes, but it was also a governor. You couldn't work yourself to death because the work itself imposed limits.

AI removed the governor. Now the only limit is your cognitive endurance. And most people don't know their cognitive limits until they've blown past them.

The engineers who thrive in this era won't be the ones who use AI the most. They'll be the ones who use it the most wisely.

If you're tired, it's not because you're doing it wrong. It's because this is genuinely hard. The tool is new, the patterns are still forming, and the industry is pretending that more output equals more value. It doesn't. Sustainable output does.

Take care of your brain. It's the only one you've got.

---

## References

- [Original article by Siddhant Khare on AI fatigue and the productivity paradox](https://siddhantkhare.com/writing/ai-fatigue-is-real)
- [Discussion on Hacker News about AI fatigue in engineering](https://news.ycombinator.com/item?id=46934404)
- [David Crawshaw's perspective on eight months of working with AI agents](https://crawshaw.io/blog/eight-more-months-of-agents)
